train_mode: "finetune"

training_model_parameters:
    goal_loss_weight: 1
    collision_loss_weight: 10

data_module_parameters:
  data_dir: /data/mpinets_hybrid_training_data_small #/data/mpinets_hybrid_training_data_small
  trajectory_key: 'global_solutions' # Could also use 'global_solutions' for the global expert
  num_obstacle_points: 4096
  num_target_points: 128
  random_scale: 0

shared_parameters:
    num_robot_points: 2048

model_path: /workspace/mpinets_hybrid_expert.ckpt
checkpoint_interval: 1
validation_interval: 200
gpus: 4 # If > 1, will use DDP for training.
batch_size: 128
save_checkpoint_dir: /workspace/mpinets_finetune_b128
experiment_name: finetune_mpinets_hybrid_training_data_small_mpinets_hybrid_expert_b128
description: "finetune training"
